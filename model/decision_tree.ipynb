{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib  # Import joblib to save the model\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Function to preprocess the JSON data\n",
    "def preprocess_json_data(collection):\n",
    "    X = []\n",
    "    y = []\n",
    "    feature_names = [\n",
    "        'tcp_syn_flag_ratio',\n",
    "        'udp_port_entropy',\n",
    "        'avg_pkt_size',\n",
    "        'flow_density',\n",
    "        'ip_entropy'\n",
    "    ]\n",
    "\n",
    "    total_records = collection.count_documents({})\n",
    "\n",
    "    logging.info(f\"Processing {total_records} records from MongoDB...\")\n",
    "\n",
    "    for i, entry in enumerate(tqdm(collection.find(), total=total_records, desc=\"Processing Records\")):\n",
    "        tcp_syn_flag_ratio = (entry.get('tcp_syn_fwd_count', 0) + entry.get('tcp_syn_bwd_count', 0)) / (entry.get('fwd_packet_count', 1) + entry.get('bwd_packet_count', 1))\n",
    "        udp_port_entropy = entry.get('unique_udp_source_ports', 0) * entry.get('unique_udp_dest_ports', 0)\n",
    "        avg_pkt_size = (entry.get('avg_fwd_pkt_size', 0) + entry.get('avg_bwd_pkt_size', 0)) / 2\n",
    "        flow_density = entry.get('flow_packets_per_sec', 0) / entry.get('flow_bytes_per_sec', 1)\n",
    "        ip_entropy = entry.get('source_ip_entropy', 0) + entry.get('dest_port_entropy', 0)\n",
    "\n",
    "        X.append([tcp_syn_flag_ratio, udp_port_entropy, avg_pkt_size, flow_density, ip_entropy])\n",
    "        label = entry.get('label')\n",
    "        y.append(0 if label == 'BENIGN' else 1 if label == 'UDP_FLOOD' else 2 if label == 'TCP_SYN_FLOOD' else -1)\n",
    "\n",
    "        # Print progress every 1000 samples\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            logging.info(f\"Processed {i} records...\")\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    X = imputer.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    logging.info(\"Preprocessing complete.\")\n",
    "    return imputer, scaler, X_scaled, y, feature_names\n",
    "\n",
    "# Function to predict a new sample\n",
    "def predict_sample(model, scaler, imputer, sample_data):\n",
    "    # Impute missing values using the saved imputer (transform the data)\n",
    "    sample_data_imputed = imputer.transform([sample_data])  # Shape must be (1, n_features)\n",
    "    \n",
    "    # Standardize the new sample using the saved scaler (transform the data)\n",
    "    sample_data_scaled = scaler.transform(sample_data_imputed)  # Shape must be (1, n_features)\n",
    "\n",
    "    # Make prediction using the loaded model\n",
    "    prediction = model.predict(sample_data_scaled)\n",
    "    \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    logger.info(\"Connecting to MongoDB...\")\n",
    "\n",
    "    # MongoDB connection URI\n",
    "    uri = \"mongodb://admin:b3BYFU0kJZpGNK6Dt42V@node1-bffd0a8e5302ff2a.database.cloud.ovh.net,node2-bffd0a8e5302ff2a.database.cloud.ovh.net,node3-bffd0a8e5302ff2a.database.cloud.ovh.net/admin?replicaSet=replicaset&tls=true\"\n",
    "\n",
    "    # Specify the database and collection\n",
    "    db_name = \"ddos_detection\"  # Replace with your database name\n",
    "    collection_name = \"traffic_features\"  # Replace with your collection name\n",
    "    \n",
    "    # Connect to the MongoDB cluster\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    logger.info(\"Data Preprocessing...\")\n",
    "\n",
    "    # Preprocess data and get feature names\n",
    "    imputer, scaler, X_scaled, y, feature_names = preprocess_json_data(collection)\n",
    "\n",
    "    # Save the scaler to a .pkl file\n",
    "    scaler_filename = 'scaler.pkl'\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Standard Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    # Save the imputer to a .pkl file\n",
    "    imputer_filename = 'imputer.pkl'\n",
    "    joblib.dump(imputer, imputer_filename)\n",
    "    print(f\"Imputer saved to {imputer_filename}\")\n",
    "\n",
    "    # Train-test split (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Decision Tree model\n",
    "    logger.info(\"Training the Decision Tree model...\")\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "    for _ in tqdm(range(1), desc=\"Training Progress\"):\n",
    "        tree_model.fit(X_train, y_train)\n",
    "\n",
    "    logger.info(\"Training complete!\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = tree_model.predict(X_test)\n",
    "\n",
    "    # Evaluation: Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances = tree_model.feature_importances_\n",
    "\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Print the feature importance\n",
    "    print(feature_importance_df)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in Random Forest Model')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model to a .pkl file\n",
    "    model_filename = 'decision_tree.pkl'\n",
    "    joblib.dump(tree_model, model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the client connection\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Load pickles\n",
    "    model = joblib.load(\"decision_tree.pkl\")\n",
    "    imputer = joblib.load(\"imputer.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "    # Example new sample\n",
    "    new_sample = [-0.78267391, -0.14192909, -0.11761717, -0.41677815,  2.21815411]\n",
    "\n",
    "    # Get prediction for the new sample\n",
    "    prediction = predict_sample(model, scaler, imputer, new_sample)\n",
    "\n",
    "    if len(prediction) > 0:\n",
    "        prediction = prediction[0]\n",
    "\n",
    "    print(\"Prediction for new sample:\", prediction)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
